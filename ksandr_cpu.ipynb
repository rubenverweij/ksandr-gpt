{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4848090",
   "metadata": {},
   "source": [
    "# KSANDR-GPT\n",
    "\n",
    "performance test for a CPU approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f423af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onprem import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cc9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from C:\\Users\\info\\onprem_data\\models\\zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 3900\n",
      "llama_context: n_ctx_per_seq = 3900\n",
      "llama_context: n_batch       = 1024\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (3900) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 3904 (padded)\n",
      "llama_kv_cache_unified: kv_size = 3904, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   488.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   283.63 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-beta', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650747f7",
   "metadata": {},
   "source": [
    "Encodeer documenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4574a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at C:\\Users\\info\\onprem_data\\vectordb\\dense\n",
      "Loading documents from docs/txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to process\n",
      "Split into 0 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm.ingest(source_directory=\"docs/txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90e95961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that there is only one document being referred to in this context.\n",
      "\n",
      "Therefore, I answer: There is 1 document ingested by the user."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4694.94 ms\n",
      "llama_perf_context_print: prompt eval time =    4694.65 ms /   438 tokens (   10.72 ms per token,    93.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4821.35 ms /    37 runs   (  130.31 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    9577.20 ms /   475 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 44s\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many documents are ingested by the user?',\n",
       " 'answer': 'Based on the provided context, it appears that there is only one document being referred to in this context.\\n\\nTherefore, I answer: There is 1 document ingested by the user.',\n",
       " 'source_documents': [Document(id='c9afbd27-6165-488d-82b0-f0f1e6827d24', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.24277740716934204}, page_content='Het document is bedoeld om een overzicht te geven van de transport-, montage-, inbedrijfstellings- en\\nonderhoudswerkzaamheden na levering aan de klant. Meer gedetailleerde informatie is op aanvraag verkrijgbaar bij\\nSBG Neumark. Informatie over hermetisch gesloten of met gaskussen afgesloten, vrij ademende (met conservator) en\\nlastschakelaar-units vindt u hieronder in de tekst.\\nLet erop dat wanneer dit wordt aangegeven, de informatie voor het juiste transformatortype wordt gebruikt.'),\n",
       "  Document(id='0b4d2fb9-ba99-4c21-a6dc-3589edec59c6', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Operating_Instruction_B2021_DT_en_1.txt', 'table': False, 'table_captions': '', 'score': 0.20611901540904087}, page_content='The intent of the document is to provide an overview during the transportation, assembly, commissioning, and\\nmaintenance activities performed once the unit had been released form factory. More detail information can be sourced\\nfrom our offices on request. Information on Hermitically sealed, sealed with gas cushion, Free Breathing with Conservator\\nand OLTC units is provided below.'),\n",
       "  Document(id='53eb15ee-53b6-4d7b-b567-295f8396de41', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/distribution_transformers_en_8.txt', 'table': False, 'table_captions': '', 'score': 0.19756388664245605}, page_content='Phone +33 251 532200\\n                                                                     Phone +27 21 505 3000'),\n",
       "  Document(id='1eacdf6a-d2cd-4d72-8cbc-4b3e147ee596', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.16398292779922485}, page_content='BEDIENINGSHANDLEIDING B / 2021\\n\\n\\n                  TRANSPORT, INSTALLATIE; INBEDRIJFSTELLING EN ONDERHOUD VAN\\n                  DISTRIBUTIETRANSFORMATOREN MET HERMETISCH GESLOTEN KETEL\\n              Opgelet: de eenheid is drukloos bij de in het testprotocol aangegeven referentietemperatuur\\n                                 â€“ geldt alleen voor hermetisch afgesloten eenheden â€“')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.ask(\"How many documents are ingested by the user?\", score_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b843c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 749 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, based on the context provided, it is stated in point 3.8 that transformers with different overbrengingsverhoudingen (transformation ratios) can be delivered, and the required ratio can be set according to the switching scheme, marking, and type plate. So yes, transformatoren can have different transformation ratios."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19753.77 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9533.42 ms /    75 runs   (  127.11 ms per token,     7.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    9611.03 ms /    76 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Kunnen transformatoren verschillende overbrengingsverhoudingen hebben?',\n",
       " 'answer': '\\nYes, based on the context provided, it is stated in point 3.8 that transformers with different overbrengingsverhoudingen (transformation ratios) can be delivered, and the required ratio can be set according to the switching scheme, marking, and type plate. So yes, transformatoren can have different transformation ratios.',\n",
       " 'source_documents': [Document(id='77d06660-b898-4067-968f-558d2ef340aa', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_2.txt', 'table': False, 'table_captions': '', 'score': 0.712337076663971}, page_content='controleer deze.\\n   3.8 Wanneer transformatoren met verschillende overbrengingsverhoudingen worden geleverd, kan de vereiste\\n\\t\\t      overbrengingsverhouding overeenkomstig het schakelschema, de markering en het typeplaatje worden ingesteld.\\n\\t\\t      Zorg ervoor dat de juiste spanningsverhouding, in overeenstemming met de voedingsspanning, wordt gekozen.\\n   3.9 Controleer en ontlucht bij units die niet hermetisch zijn afgesloten het buchholzrelais, de doorvoeringen en de\\n\\t\\t radiatoren.'),\n",
       "  Document(id='e42cd66a-0bf2-4474-853c-39afa17259d4', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.4693912863731384}, page_content='wikkelingen en de olie worden berekend voor elk keteltype. Om deze reden adviseren wij om na levering van de\\n   transformator, de olievuldop NIET te openen, te ontluchten of de doorvoeringen te ontluchten.\\n   Voor alle werkzaamheden waarbij de transformatoren moeten worden geopend, bijv. installatie van een overdrukventiel\\n   of andere bewakingsapparatuur, de vervanging van doorvoeringen en/of afdichtingen, neem dan de aanwijzingen in'),\n",
       "  Document(id='0e7d14c2-1642-48d3-8d29-f3866e6688de', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_3.txt', 'table': False, 'table_captions': '', 'score': 0.4564012289047241}, page_content='4.6 Olie bijvullen\\n\\t\\t Na voltooiing van de werkzaamheden moeten de transformatoren worden gevuld.\\n\\t\\t     Ga bij hermetisch gesloten units als volgt te werk:\\n\\t\\t     4.6.1 Schroef de dop van de vulbuis.\\n\\t\\t     4.6.2 Vul de transformatoren en de vulbuis met olie.\\n\\t\\t     4.6.3 Ontluchten van de doorvoeringen.\\n\\t\\t     4.6.4. Vul de vulbuis opnieuw (tot aan de rand) en sluit hem af met het deksel. Zorg ervoor dat alle\\n\\t\\t\\t           andere units zijn gevuld (indien nodig) en sluit ze tot slot af.'),\n",
       "  Document(id='68c226cd-7b14-4186-90b1-1bf0c10c0b75', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.45214831829071045}, page_content='2. Transport\\n   2.1 Voor het wegtransport moeten altijd voertuigen met luchtvering worden gebruikt.\\n   2.2 Bij aankomst van de goederen moet eventuele transportschade worden gemeld aan de vervoerder die\\n\\t\\t     verantwoordelijk is voor de levering.\\n   2.3 Lakschade moet onmiddellijk worden hersteld.\\n   2.4 Gebruik alleen de hijsogen aan de bovenzijde van het deksel om de transformator op te tillen/neer te laten.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask(\"Kunnen transformatoren verschillende overbrengingsverhoudingen hebben?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1ead7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 68 prefix-match hit, remaining 389 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the ingested document context, here is a summary:\n",
      "\n",
      "The document provides an overview of the activities performed during transportation, assembly, commissioning, and maintenance of units. The document highlights the importance of following specific procedures to ensure the quality and safety of the units.\n",
      "\n",
      "Some key points mentioned in the document include:\n",
      "\n",
      "* The use of solid insulation materials and plant processes for steel-sheet production.\n",
      "* The application of a coating process using paint via immersion (standard RAL 7033).\n",
      "* The assembly and mounting of components in a short-circuit proof manner using pressed steel sheets.\n",
      "* The use of environmentally friendly hydro-based coating systems.\n",
      "\n",
      "Overall, the document appears to be focused on providing detailed information about the production processes and quality control measures used during the manufacturing of units."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =    4094.64 ms /   389 tokens (   10.53 ms per token,    95.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20383.72 ms /   154 runs   (  132.36 ms per token,     7.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   24758.77 ms /   543 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ingesting the document context, I can provide a summary of the main points as follows:\n",
      "\n",
      "**Purpose and Scope**: The document provides an overview of the transportation, assembly, commissioning, and maintenance activities performed on a unit after it has been released from the factory.\n",
      "\n",
      "**Detailed Information**: For more detailed information, readers are directed to contact SBG Neumark offices for further assistance.\n",
      "\n",
      "**Components and Processes**: The document describes various components, such as steel sheets with specific requirements, and processes like deep-drawing, welding, and painting.\n",
      "\n",
      "**Quality Control and Corrosion Protection**: The document highlights the importance of quality control measures, such as maximum impregnation and state-of-the-art corrugation. It also emphasizes the need for corrosion protection using various methods, including hydro-based coating systems."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21540.77 ms /   162 runs   (  132.97 ms per token,     7.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   21838.44 ms /   163 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ingested document context appears to be a technical document related to the manufacturing and maintenance of electrical transformers.\n",
      "\n",
      "The document provides an overview of the transportation, assembly, commissioning, and maintenance activities performed on these units.\n",
      "\n",
      "It also outlines the specific requirements for different types of transformer components, including solid insulation materials, plant processes, steel-sheet quality, and coating requirements.\n",
      "\n",
      "Additionally, the document discusses the process of drying and oil filling under vacuum as a basis for maximum impregnation of electrical insulating materials.\n",
      "\n",
      "Overall, the ingested document context appears to be focused on providing detailed technical information related to the design, manufacturing, testing, installation, operation, maintenance, and repair of transformers."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18328.36 ms /   137 runs   (  133.78 ms per token,     7.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   18568.27 ms /   138 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the ingested document context, it appears to be a technical document related to transformers and their manufacturing process.\n",
      "\n",
      "The document seems to provide an overview of the transportation, assembly, commissioning, and maintenance activities performed on the transformers after they have been released from the factory.\n",
      "\n",
      "Additionally, the document mentions specific details about the components used in the transformers, such as insulating materials, plant processes, and corrosion protection systems."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12019.84 ms /    85 runs   (  141.41 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   12173.36 ms /    86 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ingested document context is about the overview of transportation, assembly, commissioning, and maintenance activities for a unit released from the factory. The document also provides information on specific components, manufacturing processes, and corrosion protection methods.\n",
      "\n",
      "Note: I've tried to provide a concise summary while still maintaining accuracy based on the provided context."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9949.90 ms /    66 runs   (  150.76 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   10079.29 ms /    67 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the ingested document context, here is a summary:\n",
      "\n",
      "The document provides an overview of the transportation, assembly, commissioning, and maintenance activities performed after a unit has been released from the factory.\n",
      "\n",
      "The document highlights specific components, such as hermetically sealed units, units with gas cushions, free-breathing units with conservators, and OLTC units.\n",
      "\n",
      "The document also mentions various processes, such as corrugation, coating, welding, drying, and oil filling. Additionally, it highlights the importance of following specific specifications and standards, such as PD (Pressure Dynamics) specifications."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   17764.90 ms /   119 runs   (  149.28 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   18011.22 ms /   120 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the ingested document context, here is a summary:\n",
      "\n",
      "The document provides an overview of the transportation, assembly, commissioning, and maintenance activities for a unit after it has been released from the factory.\n",
      "\n",
      "The document also provides detailed information about hermetically sealed or gas cushion-sealed units, as well as free-breathing units with conservators and OLTC (On Load Tap Changer) units.\n",
      "\n",
      "Overall, the document appears to be an instruction manual or user guide for the maintenance and operation of a specific type of unit."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   16205.54 ms /   108 runs   (  150.05 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   16428.10 ms /   109 tokens\n",
      "Llama.generate: 456 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a summarized overview:\n",
      "\n",
      "**Purpose:** The document provides an overview of activities related to transportation, assembly, commissioning, and maintenance of a unit.\n",
      "\n",
      "**Information Provided:**\n",
      "\n",
      "* Overview of processes involved in handling the unit.\n",
      "* Information on various types of units, including:\n",
      "\t+ Hermetically sealed units\n",
      "\t+ Units sealed with gas cushion\n",
      "\t+ Free-breathing units with conservator\n",
      "\t+ OLTC (On Load Tap Changer) units\n",
      "* Details on components and materials used, such as:\n",
      "\t+ Insulating materials\n",
      "\t+ Plant processes for steel-sheet production\n",
      "\t+ Requirements for deep-drawing quality\n",
      "\n",
      "**Document Structure:** The document is divided into sections that describe the various activities involved in handling the unit.\n",
      "\n",
      "Please note that this summary is based on a machine translation of the original text, which may have introduced minor errors or discrepancies."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4677.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   25744.36 ms /   183 runs   (  140.68 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   26097.77 ms /   184 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.7 s ± 5.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "llm.ask(\"Please summarise the ingested document context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a360fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 68 prefix-match hit, remaining 538 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De documenten die inzicht geven in de bediening van transformatoren zijn de secties 4.6, 3.3 en 3.4 in het document.\n",
      "\n",
      "In deze secties wordt beschreven hoe de transformatoren moeten worden gevuld met olie, en welke werkzaamheden nodig zijn om de transformatoren bedienbaar te maken."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7109.82 ms\n",
      "llama_perf_context_print: prompt eval time =    5782.05 ms /   538 tokens (   10.75 ms per token,    93.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10962.59 ms /    83 runs   (  132.08 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   16887.22 ms /   621 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Welke documenten geven inzicht in de bediening van transformatoren?',\n",
       " 'answer': 'De documenten die inzicht geven in de bediening van transformatoren zijn de secties 4.6, 3.3 en 3.4 in het document.\\n\\nIn deze secties wordt beschreven hoe de transformatoren moeten worden gevuld met olie, en welke werkzaamheden nodig zijn om de transformatoren bedienbaar te maken.',\n",
       " 'source_documents': [Document(id='c9afbd27-6165-488d-82b0-f0f1e6827d24', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.6511595845222473}, page_content='Het document is bedoeld om een overzicht te geven van de transport-, montage-, inbedrijfstellings- en\\nonderhoudswerkzaamheden na levering aan de klant. Meer gedetailleerde informatie is op aanvraag verkrijgbaar bij\\nSBG Neumark. Informatie over hermetisch gesloten of met gaskussen afgesloten, vrij ademende (met conservator) en\\nlastschakelaar-units vindt u hieronder in de tekst.\\nLet erop dat wanneer dit wordt aangegeven, de informatie voor het juiste transformatortype wordt gebruikt.'),\n",
       "  Document(id='0e7d14c2-1642-48d3-8d29-f3866e6688de', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_3.txt', 'table': False, 'table_captions': '', 'score': 0.6460481286048889}, page_content='4.6 Olie bijvullen\\n\\t\\t Na voltooiing van de werkzaamheden moeten de transformatoren worden gevuld.\\n\\t\\t     Ga bij hermetisch gesloten units als volgt te werk:\\n\\t\\t     4.6.1 Schroef de dop van de vulbuis.\\n\\t\\t     4.6.2 Vul de transformatoren en de vulbuis met olie.\\n\\t\\t     4.6.3 Ontluchten van de doorvoeringen.\\n\\t\\t     4.6.4. Vul de vulbuis opnieuw (tot aan de rand) en sluit hem af met het deksel. Zorg ervoor dat alle\\n\\t\\t\\t           andere units zijn gevuld (indien nodig) en sluit ze tot slot af.'),\n",
       "  Document(id='e42cd66a-0bf2-4474-853c-39afa17259d4', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.6169207692146301}, page_content='wikkelingen en de olie worden berekend voor elk keteltype. Om deze reden adviseren wij om na levering van de\\n   transformator, de olievuldop NIET te openen, te ontluchten of de doorvoeringen te ontluchten.\\n   Voor alle werkzaamheden waarbij de transformatoren moeten worden geopend, bijv. installatie van een overdrukventiel\\n   of andere bewakingsapparatuur, de vervanging van doorvoeringen en/of afdichtingen, neem dan de aanwijzingen in'),\n",
       "  Document(id='885c2314-fc36-4df8-bad1-7449738b052a', metadata={'document_title': '', 'extension': 'txt', 'markdown': False, 'ocr': False, 'page': -1, 'source': 'c:/Datapreds/projects/ksandr-gpt/docs/txt/Bedieningshandleiding_B2021_DT_NL_1.txt', 'table': False, 'table_captions': '', 'score': 0.6142002940177917}, page_content='3.3.3 Plaats de ontluchter.\\n\\t\\t      3.3.4 Vul het oliereservoir van de ontluchter tot het gewenste peil (oliemarkeringen aanwezig).\\n   3.4 Aard de transformator aan de aardingsschroef.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.ask(\"Welke documenten geven inzicht in de bediening van transformatoren?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0494cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "###CPU\n",
    "start_time = time.time()\n",
    "a = torch.ones(4000,4000)\n",
    "for _ in range(10000):\n",
    "    a += a\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print('CPU time = ',elapsed_time)\n",
    "\n",
    "###GPU\n",
    "torch.cuda.synchronize() \n",
    "start_time = time.time()\n",
    "b = torch.ones(4000,4000).cuda()\n",
    "for _ in range(10000):\n",
    "    b += b\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print('GPU time = ',elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d625b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
