import asyncio
import time
import uuid
import os
from datetime import datetime

from neo4j_langchain.cypher_queries import query_neo4j
from templates import TEMPLATES, SYSTEM_PROMPT
from helpers import (
    maak_metadata_filter,
    COMPONENTS,
    uniek_antwoord,
    get_embedding_function,
    vind_relevante_context,
    maak_chroma_filter,
    trim_context_to_fit,
)
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Dict, Optional, Union, List
from langchain_chroma import Chroma
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import BaseCallbackHandler


# Configuratie voor gelijktijdige verwerking van verzoeken
request_queue = asyncio.Queue()
semaphore = asyncio.Semaphore(5)
app = FastAPI()
request_responses = {}

# Configuratievariabelen
CONFIG = {
    "TEMPERATURE": float(os.getenv("TEMPERATURE", 0.2)),
    "SOURCE_MAX": int(os.getenv("SOURCE_MAX", 10)),
    "SOURCE_MAX_RERANKER": int(os.getenv("SOURCE_MAX_RERANKER", 0)),
    "SCORE_THRESHOLD": float(os.getenv("SCORE_THRESHOLD", 1.1)),
    "INCLUDE_FILTER": int(os.getenv("INCLUDE_FILTER", 1)),
    "MAX_TOKENS": int(os.getenv("MAX_TOKENS", 750)),
    "MAX_CTX": int(os.getenv("MAX_CTX", 8000)),
    "INCLUDE_SUMMARY": int(os.getenv("INCLUDE_SUMMARY", 0)),
    "INCLUDE_KEYWORDS": int(os.getenv("INCLUDE_KEYWORDS", 0)),
    "DEFAULT_MODEL_PATH": str(
        os.getenv(
            "DEFAULT_MODEL_PATH",
            "/root/.cache/huggingface/hub/models--unsloth--Qwen3-30B-A3B-Instruct-2507-GGUF/snapshots/eea7b2be5805a5f151f8847ede8e5f9a9284bf77/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf",
        )
    ),
}

model = os.path.basename(CONFIG["DEFAULT_MODEL_PATH"])
DEFAULT_QA_PROMPT = TEMPLATES[model]["DEFAULT_QA_PROMPT"]
EVALUATIE_PROMPT = TEMPLATES[model]["EVALUATIE_PROMPT"]
DEFAULT_QA_PROMPT_SIMPLE = TEMPLATES[model]["DEFAULT_QA_PROMPT_SIMPLE"]
CHROMA_PATH = "/root/onprem_data/chroma"

# Initialisatie van het taalmodel
LLM = LlamaCpp(
    model_path=CONFIG["DEFAULT_MODEL_PATH"],
    max_tokens=CONFIG["MAX_TOKENS"],
    n_gpu_layers=-1,
    n_ctx=CONFIG["MAX_CTX"],
    verbose=False,
    streaming=True,
    temperature=CONFIG["TEMPERATURE"],
    top_p=0.9,
)
embedding_function = get_embedding_function()
db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)


print(f"Starting container with {CONFIG}")


# Streaming handler
class StreamingResponseCallback(BaseCallbackHandler):
    def __init__(self, request_id: str):
        self.request_id = request_id
        self.partial_response = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """Callback method for when a new token is generated by the model."""
        self.partial_response += token
        if self.request_id in request_responses:
            request_responses[self.request_id]["partial_response"] = (
                self.partial_response
            )

    def on_llm_end(self, output: str, **kwargs):
        """Callback method for when the model has finished generating."""
        if self.request_id in request_responses:
            request_responses[self.request_id]["status"] = "completed"
            request_responses[self.request_id]["response"] = self.partial_response


# Vraagmodel
class AskRequest(BaseModel):
    prompt: str
    permission: Optional[Dict[str, Union[Dict[str, List[int]], List[int], bool]]] = None
    user_id: Optional[str] = "123"
    rag: Optional[int] = 1

    class Config:
        extra = "allow"  # Sta extra velden toe


class EvaluationRequest(BaseModel):
    expected: str
    actual: str


class ContextRequest(BaseModel):
    prompt: str


class Neo4jRequest(BaseModel):
    question: str
    template: Optional[str]


def ask_llm(
    prompt: str, chroma_filter: Optional[Dict | None], model: LlamaCpp, rag: int
):
    if rag:
        time_start = time.time()
        document_search = maak_chroma_filter(
            question=prompt, include_nouns=CONFIG["INCLUDE_KEYWORDS"]
        )
        time_doc_search = time.time()
        neo_context_text, results_new_schema = query_neo4j(prompt, chroma_filter)
        time_stages = {}
        summary = ""
        if not neo_context_text:
            context_text, results, summary, time_stages = vind_relevante_context(
                prompt=prompt,
                filter_chroma=chroma_filter,
                db=db,
                source_max_reranker=CONFIG["SOURCE_MAX_RERANKER"],
                source_max_dense=CONFIG["SOURCE_MAX"],
                score_threshold=CONFIG["SCORE_THRESHOLD"],
                where_document=document_search,
                include_summary=CONFIG["INCLUDE_SUMMARY"],
            )
            results_new_schema = []
            for doc, score in results:
                doc_dict = {
                    "id": doc.id,
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                    "type": doc.type,
                }
                doc_dict["metadata"]["score"] = score
                results_new_schema.append(doc_dict)
        else:
            context_text = neo_context_text
        time_build_context = time.time()
        available_tokens_for_context, trimmed_context_text = trim_context_to_fit(
            model=model.client,
            template=DEFAULT_QA_PROMPT,
            context_text=context_text,
            question=prompt,
            n_ctx=CONFIG["MAX_CTX"],
            max_tokens=CONFIG["MAX_TOKENS"],
        )
        time_reranker_trimming = time.time()
        prompt_with_template = DEFAULT_QA_PROMPT.format(
            system_prompt=SYSTEM_PROMPT, context=trimmed_context_text, question=prompt
        )
    else:
        prompt_with_template = DEFAULT_QA_PROMPT_SIMPLE.format(
            system_prompt=SYSTEM_PROMPT, question=prompt
        )
        results_new_schema = None
        document_search = None
    # Monitor time stages
    time_stages.update(
        {
            "maak_chroma_filter": time_doc_search - time_start,
            "vind_relevante_context": time_build_context - time_doc_search,
            "trim_context_to_fit": time_reranker_trimming - time_build_context,
        }
    )
    return {
        "question": prompt,
        "answer": model.invoke(prompt_with_template),
        "prompt": prompt_with_template,
        "source_documents": results_new_schema,
        "where_document": document_search,
        "summary": summary,
        "available_tokens_for_context": available_tokens_for_context,
        "time_stages": time_stages,
    }


# Verwerkt het verzoek en haalt de reactie op
async def process_request(request: AskRequest):
    """Process a request asynchronously and stream the result."""
    time_start = time.time()
    if CONFIG["INCLUDE_FILTER"]:
        active_filter = maak_metadata_filter(
            request=request, componenten_dict=COMPONENTS
        )
    else:
        active_filter = None
    time_vind_component = time.time()

    try:
        # Pass request_id for tracking the streaming response
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: ask_llm(
                prompt=request.prompt,
                model=LLM,
                chroma_filter=active_filter,
                rag=request.rag,
            ),
        )
        time_ask_llm = time.time()
        response["active_filter"] = str(active_filter)
        response["answer"] = uniek_antwoord(response["answer"])
        time_uniek_antwoord = time.time()
        response["time_stages"].update(
            {
                "vind_relevante_componenten": time_vind_component - time_start,
                "ask_llm": time_ask_llm - time_vind_component,
                "uniek_antwoord": time_uniek_antwoord - time_ask_llm,
            }
        )
        if request.rag:
            if not response.get("source_documents"):
                response["answer"] = (
                    "Op basis van de informatie die ik tot mijn beschikking heb, weet ik het antwoord helaas niet."
                )
        return response
    except Exception as e:
        return {"error": str(e), "filter": active_filter}


# Worker voor het verwerken van verzoeken in de wachtrij
async def request_worker():
    """Verwerkt verzoeken één voor één."""
    while True:
        request = await request_queue.get()
        async with semaphore:
            response = await process_request(request)
            end_time = time.time()
            duration = end_time - request_responses[request.id]["start_time"]
            request_responses[request.id].update(
                {
                    "status": "completed",
                    "response": response,
                    "end_time": end_time,
                    "time_duration": duration,
                }
            )


@app.post("/ask")
async def ask(request: AskRequest):
    """Verwerkt binnenkomende verzoeken."""
    request.id = str(uuid.uuid4())  # Genereer een uniek ID
    await request_queue.put(request)  # Voeg het verzoek toe aan de wachtrij
    start_time = time.time()
    in_queue = request_queue.qsize()
    request_responses[request.id] = {
        "status": "processing",
        "start_time": start_time,
        "in_queue_start": in_queue,
        "start_time_formatted": datetime.fromtimestamp(start_time).strftime(
            "%H:%M:%S %d-%m-%Y"
        ),
    }
    return {
        "message": "Verzoek wordt verwerkt",
        "request_id": request.id,
        "in_queue_start": in_queue,
        "start_time": datetime.fromtimestamp(start_time).strftime("%H:%M:%S %d-%m-%Y"),
    }


@app.get("/status/{request_id}")
async def get_status(request_id: str):
    """Check the status and the result of a request."""
    if request_id in request_responses:
        response_data = request_responses[request_id]
        if response_data["status"] == "completed":
            return response_data
        elif "partial_response" in response_data:
            return {
                "status": "processing",
                "start_time_formatted": response_data["start_time_formatted"],
                "in_queue_start": response_data["in_queue_start"],
                "partial_response": response_data["partial_response"],
                "in_queue_current": await get_request_position_in_queue(
                    request_id=request_id
                ),
            }
        return {
            "status": "processing",
            "start_time_formatted": response_data["start_time_formatted"],
            "in_queue_start": response_data["in_queue_start"],
            "in_queue_current": await get_request_position_in_queue(
                request_id=request_id
            ),
        }
    return {"message": "Request not found", "status": "not_found"}


@app.post("/evaluate")
def evaluate(req: EvaluationRequest):
    result = evaluate_answer(req.expected, req.actual)
    return {"evaluation": result}


@app.post("/context")
def context(req: ContextRequest):
    return {
        "answer": LLM.invoke(req.prompt),
    }


def evaluate_answer(expected: str, actual: str) -> str:
    """Laat het LLM de antwoorden vergelijken en bepalen of het correct is."""
    comparison_prompt = EVALUATIE_PROMPT.format(actual=actual, expected=expected)
    result = LLM(comparison_prompt)
    return result.strip()


def get_image_name() -> str:
    return os.getenv("IMAGE_NAME", "Unknown")


@app.get("/metadata")
def get_metadata():
    CONFIG["image_name"] = get_image_name()
    return CONFIG


@app.on_event("startup")
async def startup():
    """Start de worker om verzoeken sequentieel te verwerken."""
    asyncio.create_task(request_worker())


async def get_request_position_in_queue(request_id: str) -> int:
    """Calculate the real-time position of the request in the queue."""
    queue_list = list(request_queue._queue)
    for index, queued_request in enumerate(queue_list):
        if queued_request.id == request_id:
            return index + 1
    return 0
