"""
Main entry point for the Ksandr platform API and LLM services.

This module initializes the FastAPI application, sets up the LLMManager,
imports core utilities, and integrates document processing, graph-based querying,
and dynamic prompt templating for the Dutch Ksandr infrastructure platform.

APIs include question-answering endpoints, summarization, metadata retrieval, and file upload handlers.
Utilities for reference linking, context trimming, and query construction are available.
"""

import asyncio
import time
import uuid
import os
import re

import logging
from datetime import datetime
from fastapi import FastAPI

from pathlib import Path
from ksandr.settings.templates import (
    TEMPLATES,
    SYSTEM_PROMPT,
    dynamische_prompt_elementen,
)
from ksandr.support.llm import LLMManager, RecursiveSummarizer
from ksandr.settings.refs import replace_patterns
from ksandr.support.graph import build_cypher_query, check_for_nbs, match_query_by_tags
from ksandr.settings.config import SECRETS
from ksandr.support.helpers import (
    create_metadata_filter,
    COMPONENTS,
    get_embedding_function,
    find_relevant_sources,
    create_chroma_filter,
    trim_context_to_fit,
    get_aad_based_on_question,
    detect_aad,
    detect_location,
    source_document_dummy,
    is_valid_sentence,
    summary_request,
    get_summary,
    build_links,
    is_llm_appropiate,
    clean_for_llm,
    text_quality_metrics,
    AskRequest,
    ContextRequest,
    LLMRequest,
    FileRequest,
)
from fastapi import HTTPException
from typing import Dict, Optional, List, Any
from langchain_chroma import Chroma
from langchain_core.callbacks import BaseCallbackHandler
from langchain_neo4j import Neo4jGraph

from fastapi import UploadFile, File
from io import BytesIO

from PyPDF2 import PdfReader


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


GRAPH = Neo4jGraph(
    url="bolt://neo4j-staging:7687",
    username=SECRETS.get("username"),
    password=SECRETS.get("password"),
)

# Configuratie voor gelijktijdige verwerking van verzoeken
request_queue = asyncio.Queue()
semaphore = asyncio.Semaphore(5)
app = FastAPI()
request_responses = {}

# Configuratievariabelen
CONFIG = {
    "TEMPERATURE": float(os.getenv("TEMPERATURE", 0.7)),
    "SOURCE_MAX": int(os.getenv("SOURCE_MAX", 5)),
    "SOURCE_MAX_RERANKER": int(os.getenv("SOURCE_MAX_RERANKER", 0)),
    "SCORE_THRESHOLD": float(os.getenv("SCORE_THRESHOLD", 1.1)),
    "INCLUDE_FILTER": int(os.getenv("INCLUDE_FILTER", 1)),
    "MAX_TOKENS": int(os.getenv("MAX_TOKENS", 750)),
    "MAX_CTX": int(os.getenv("MAX_CTX", 4096)),
    "INCLUDE_SUMMARY": int(os.getenv("INCLUDE_SUMMARY", 0)),
    "INCLUDE_KEYWORDS": int(os.getenv("INCLUDE_KEYWORDS", 0)),
    "DEFAULT_MODEL_PATH": str(
        os.getenv(
            "DEFAULT_MODEL_PATH",
            "/root/huggingface/hub/models--unsloth--Qwen3-30B-A3B-Instruct-2507-GGUF/snapshots/eea7b2be5805a5f151f8847ede8e5f9a9284bf77/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf",
        )
    ),
    "INCLUDE_PERMISSION": int(os.getenv("INCLUDE_PERMISSION", 1)),
    "CHROMA_PATH": os.getenv("CHROMA_PATH", "/root/onprem_data/chroma"),
    "CHROMA_PATH_CYPHER": os.getenv(
        "CHROMA_PATH_CYPHER", "/root/onprem_data/chroma_cypher"
    ),
}

model = os.path.basename(CONFIG["DEFAULT_MODEL_PATH"])
DEFAULT_QA_PROMPT = TEMPLATES[model]["DEFAULT_QA_PROMPT"]
CYPHER_PROMPT = TEMPLATES[model]["CYPHER_PROMPT"]
DEFAULT_QA_PROMPT_SIMPLE = TEMPLATES[model]["DEFAULT_QA_PROMPT_SIMPLE"]
LOCATION_QA_PROMPT = TEMPLATES[model]["LOCATION_QA_PROMPT"]
SUMMARY_PROMPT_INITIAL = TEMPLATES[model]["SUMMARY_PROMPT_INITIAL"]
SUMMARY_PROMPT_PARTIAL = TEMPLATES[model]["SUMMARY_PROMPT_PARTIAL"]
SUMMARY_PROMPT_CONCLUDE = TEMPLATES[model]["SUMMARY_PROMPT_CONCLUDE"]
SUMMARY_PROMPT_CORRECTION = TEMPLATES[model]["SUMMARY_PROMPT_CORRECTION"]
SUMMARY_PROMPT_SINGLE = TEMPLATES[model]["SUMMARY_PROMPT_SINGLE"]
SUMMARY_PROMPT = TEMPLATES[model]["SUMMARY_PROMPT"]

# Initialisatie van het taalmodel
LLM_MANAGER = LLMManager(
    model_path=CONFIG["DEFAULT_MODEL_PATH"],
    max_tokens=CONFIG["MAX_TOKENS"],
    n_gpu_layers=-1,
    temperature=CONFIG["TEMPERATURE"],
    top_p=0.9,
)
LLM_MANAGER.load_llm(n_ctx=CONFIG["MAX_CTX"])
LLM_MANAGER.get_llm()("Hello world", max_tokens=1)

embedding_function = get_embedding_function()
db = Chroma(
    persist_directory=CONFIG["CHROMA_PATH"], embedding_function=embedding_function
)
db_cypher = Chroma(
    persist_directory=CONFIG["CHROMA_PATH_CYPHER"],
    embedding_function=embedding_function,
)
print(f"Starting container with {CONFIG}")


class StreamingResponseCallback(BaseCallbackHandler):
    """
    Callback handler for streaming responses generated by the LLM.

    This class handles partial responses generated token-by-token from the language model,
    allowing them to be streamed back to the client. It stores incomplete answers during generation
    for the unique request identified by `request_id`.
    """

    def __init__(self, request_id: str):
        self.request_id = request_id
        self.partial_response = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.partial_response += token
        # Store partial result
        if self.request_id in request_responses:
            request_responses[self.request_id]["partial_response"] = (
                self.partial_response
            )


async def async_stream_generator(sync_gen):
    """
    Fully async wrapper for a synchronous generator.
    Yields items without using threads.
    """
    for item in sync_gen:
        await asyncio.sleep(0.001)  # yield control to event loop
        yield item


def process_ask(request: AskRequest) -> dict:
    """
    Processes an AskRequest and streams partial responses.

    This function takes an AskRequest object, determines whether the request is for summarization,
    handles the logic for applying filters, generates context and prompt data,
    and coordinates the LLM response stream. It also manages callback handling for
    streaming partial results to the client and collects output in a structured dictionary.
    """
    if summary_request(request.prompt):
        return {
            "question": request.prompt,
            "answer": get_summary(request.prompt),
            "prompt": "",
            "active_filter": "",
            "source_documents": [],
            "time_stages": {},
        }

    database_filter = (
        create_metadata_filter(request, COMPONENTS, CONFIG["INCLUDE_PERMISSION"])
        if CONFIG["INCLUDE_FILTER"]
        else None
    )
    callback = StreamingResponseCallback(request.id)
    if request.prompt.startswith("!"):
        request.rag = 0

    # Retrieve the correct template and reference docs
    prompt_with_template, reference_docs, time_stages = build_prompt_template(
        chroma_filter=database_filter, request=request
    )

    stream = LLM_MANAGER.get_llm().client(
        prompt_with_template, stream=True, max_tokens=1500
    )
    full_answer = ""
    buffer = ""
    sentence_end_re = re.compile(r"[.!?]")
    seen_sentences = []
    for chunk in stream:
        token = chunk["choices"][0]["text"]
        full_answer += token
        buffer += token
        callback.on_llm_new_token(token)

        if not sentence_end_re.search(token):
            continue

        # Only check the last sentence is a replication
        sentences = re.split(r"(?<=[.!?])\s*", buffer)
        completed = sentences[:-1]
        buffer = sentences[-1]
        if not completed:
            continue

        # We only check the LAST completed sentence
        last_two_seen = seen_sentences[-2:]
        last_sentence = completed[-1].strip()
        last_sentence_no_digits = re.sub(r"\d+", "", last_sentence)
        if (
            last_sentence_no_digits
            and is_valid_sentence(last_sentence_no_digits)
            and last_sentence_no_digits in last_two_seen
        ):
            logging.info(f"Detected duplicate sentence: {last_sentence}")
            final_answer = full_answer.replace(last_sentence, "").strip()
            if len(final_answer) == 0:
                final_answer = last_sentence
            final_answer = re.sub(r"\n+\s*\d+[\.\)\-]\s*$", "", final_answer)
            return {
                "question": request.prompt,
                "answer": replace_patterns(final_answer),
                "prompt": prompt_with_template,
                "active_filter": str(database_filter),
                "source_documents": reference_docs,
                "time_stages": time_stages,
            }
        seen_sentences.append(last_sentence_no_digits)

    # Generator klaar, final answer
    final_answer = replace_patterns(full_answer)

    # FIXME
    # We could add an addiitonal control mechanism here

    # In case there is no source
    if request.rag:
        if len(reference_docs) == 0:
            final_answer = """Er is geen informatie gevonden die gebruikt kan worden bij de beantwoording.
                        
            Mogelijk heeft u geen toegang tot de gewenste informatie of is deze informatie niet beschikbaar.
            
            """

    return {
        "question": request.prompt,
        "answer": final_answer,
        "prompt": prompt_with_template,
        "active_filter": str(database_filter),
        "source_documents": reference_docs,
        "time_stages": time_stages,
    }


def process_summarize(request: FileRequest) -> dict:
    """Process a summarization request, either from in-memory content or from a file.

    This function performs the following steps:
    1. Checks whether the request contains in-memory text content or a file path.
    2. Reads and loads the content for summarization.
    3. Computes quality metrics on the input text.
    4. Decides whether the content is appropriate for LLM-based summarization based on the extracted metrics.
    5. If valid, uses the RecursiveSummarizer to generate a summary using various prompt templates.
    6. If the text extraction is not valid, returns an error summary message indicating why summarization is not possible.

    Args:
        request (FileRequest): The summarize request, with either file_path or content.

    Returns:
        dict: A dictionary containing the generated summary or an error message, suitable for downstream consumption.
    """
    text: str | None = None

    # Content or file path is provided directly
    if request.content:
        logging.info("Summarize request received with in-memory content")
        text = request.content
    elif request.file_path:
        file_path = Path(request.file_path)

        if not file_path.is_file():
            raise HTTPException(status_code=404, detail="File not found")

        try:
            text = file_path.read_text(encoding="utf-8")
            logging.info(f"Done reading text file {request.file_path}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to read file: {e}")
    # Neither content nor file_path provided
    else:
        raise HTTPException(
            status_code=400,
            detail="No content or file_path provided for summarization",
        )
    metrics = text_quality_metrics(text)
    valid_extraction = is_llm_appropiate(metrics)

    logging.info(
        f"Done reading text file {request.file_path} with metrics {metrics}, extraction valid={valid_extraction}"
    )
    if valid_extraction:
        summarizer = RecursiveSummarizer(
            llm_manager=LLM_MANAGER,
            template_initial=SUMMARY_PROMPT_INITIAL,
            template_partial=SUMMARY_PROMPT_PARTIAL,
            template_conclude=SUMMARY_PROMPT_CONCLUDE,
            template_correction=SUMMARY_PROMPT_CORRECTION,
            template_full=SUMMARY_PROMPT,
            template_single=SUMMARY_PROMPT_SINGLE,
            text=text,
        )
        summary = summarizer.summarize()
    else:
        summary_cleaned = """#### Samenvatting maken is helaas niet mogelijk

        Er kan geen samenvatting worden gemaakt omdat de tekst extractie uit de PDF van onvoldoende kwaliteit is.

        Misschien oogt het document van voldoende kwaliteit, maar de kans is aanzienlijk dat de onderliggende structuur van de PDF beschadigd is of dat er vervormde karakters in de tekst staan.

        U kunt proberen een nieuwe PDF van het document te maken via bijvoorbeeld **Microsoft Word** en deze opnieuw te uploaden.
        
        #### Tips
        - Open het originele bestand in Microsoft Word
        - Kies een gangbaar lettertype zoals Arial, Calibri of Times New Roman
        - Ga naar Bestand → Opslaan als
        - Kies bij Opslaan als type: PDF (*.pdf)
        - Klik op Opties…
            Kies:
            ✅ Standaard (publicatie online en afdrukken) → beste kwaliteit
            ❌ Niet Minimale grootte
        - Opslaan 
        
        """
        summary = summary_cleaned

    return {
        "status": "completed",
        "summary_cleaned": summary,
        "summary_raw": summary,
        "summary_length": len(summary.split()),
    }


async def request_worker():
    """
    Background worker that processes incoming requests from the queue asynchronously.

    For each request:
    - Waits for the next request from the `request_queue`.
    - Runs the appropriate processing function based on the request type
      ('ask' or 'summarize') inside a thread, guarded by a semaphore to limit concurrency.
    - Handles response recording, marking request as completed, and logging response duration.

    This function is meant to be run as a long-lived asyncio task.
    """
    while True:
        request = await request_queue.get()
        try:
            async with semaphore:
                if request.type == "ask":
                    response = await asyncio.to_thread(process_ask, request)
                elif request.type == "summarize":
                    response = await asyncio.to_thread(process_summarize, request)
        finally:
            request_queue.task_done()

        end_time = time.time()
        duration = end_time - request_responses[request.id]["start_time"]
        request_responses[request.id].update(
            {
                "status": "completed",
                "response": response,
                "end_time": end_time,
                "time_duration": duration,
            }
        )


async def get_request_position_in_queue(request_id: str) -> int:
    """
    Given a request ID, returns the 1-based position of the request in the current request queue.

    Args:
        request_id (str): The unique identifier for the queued request.

    Returns:
        int: The position of the request in the queue (1-based). Returns 0 if the request is not found.
    """
    queue_list = list(request_queue._queue)
    for index, queued_request in enumerate(queue_list):
        if queued_request.id == request_id:
            return index + 1
    return 0


def retrieve_answer_from_vector_store(
    prompt: str, chroma_filter: Optional[Dict | None]
) -> tuple:
    """
    Retrieves the answer from the vector store given a prompt and an optional chroma filter.

    This function performs the following:
    - Generates a document search filter based on the input prompt.
    - Searches for relevant source documents using the Chroma vector store, applying any provided filters.
    - Processes found documents and prepares the context to be used by the language model.
    - Trims the context if necessary to fit within model constraints.
    - Returns the answer context and supporting source documents, as well as timing metadata.

    Args:
        prompt (str): The user’s question or input for which to generate an answer.
        chroma_filter (Optional[Dict | None]): An optional filter to restrict the search space in Chroma.

    Returns:
        Tuple: Prepared context, result documents, and timing information for further answer generation.
    """
    time_start = time.time()
    document_search = create_chroma_filter(
        question=prompt, include_nouns=CONFIG["INCLUDE_KEYWORDS"]
    )
    time_doc_search = time.time()
    neo_context_text = None
    time_stages = {}
    if not neo_context_text:
        context_text, results, time_stages = find_relevant_sources(
            prompt=prompt,
            filter_chroma=chroma_filter,
            db=db,
            source_max_reranker=CONFIG["SOURCE_MAX_RERANKER"],
            source_max_dense=CONFIG["SOURCE_MAX"],
            score_threshold=CONFIG["SCORE_THRESHOLD"],
            where_document=document_search,
        )
        results_new_schema = []
        for doc, score in results:
            doc_dict = {
                "id": doc.id,
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "type": doc.type,
            }
            doc_dict["metadata"]["score"] = score
            results_new_schema.append(doc_dict)
    else:
        context_text = neo_context_text
    logging.info("Done building context")
    time_build_context = time.time()
    _, trimmed_context_text = trim_context_to_fit(
        model=LLM_MANAGER.get_llm().client,
        template=DEFAULT_QA_PROMPT,
        context_text=context_text,
        question=prompt,
        n_ctx=CONFIG["MAX_CTX"],
        max_tokens=CONFIG["MAX_TOKENS"],
    )
    if len(trimmed_context_text) < 10:
        trimmed_context_text = "Er is geen informatie gevonden die gebruikt kan worden bij de beantwoording."

    time_reranker_trimming = time.time()
    prompt_with_template = DEFAULT_QA_PROMPT.format(
        system_prompt=SYSTEM_PROMPT,
        context=trimmed_context_text,
        question=prompt,
    )
    time_stages.update(
        {
            "maak_chroma_filter": time_doc_search - time_start,
            "vind_relevante_context": time_build_context - time_doc_search,
            "trim_context_to_fit": time_reranker_trimming - time_build_context,
        }
    )
    return prompt_with_template, results_new_schema, time_stages


def retrieve_weblocation_template(question: str) -> str:
    """
    Generates a prompt template for answering location-based questions.

    This function builds the prompt for questions that require specific information
    about web locations, based on the question content. It retrieves relevant 'aads' (address
    information or similar context) using the question, formats the location links, and injects them
    into a predefined prompt template for location-based Q&A.

    Args:
        question (str): The user's question related to locations.

    Returns:
        str: The formatted prompt string ready for the language model to process.
    """
    aads = get_aad_based_on_question(question)
    prompt_with_template = LOCATION_QA_PROMPT.format(
        locations=build_links(aads),
        question=question,
    )
    return prompt_with_template


def build_prompt_template(
    request: AskRequest, chroma_filter: Optional[Dict | None]
) -> tuple:
    """
    Builds the prompt template for an AskRequest.

    This function determines the appropriate prompt template and reference documents
    to use for the given AskRequest, depending on whether retrieval-augmented generation (RAG)
    is enabled and the nature of the user's question. It supports special handling for
    location-based and 'aad' (address or access rights) related queries, leverages Neo4j
    results when available, and can fall back to a default Q&A prompt for simple cases.

    Args:
        request (AskRequest): The request object containing the user's query and parameters.
        chroma_filter (Optional[Dict | None]): Filter applied to the vector store for document retrieval.

    Returns:
        Tuple[str, list, dict]:
            - The formatted prompt string to be used by the language model,
            - The list of reference documents retrieved or dummy sources,
            - The dictionary of timings and stages for diagnostics.
    """
    reference_documents = []
    time_stages = {}
    if request.rag:
        if detect_location(request.prompt):
            prompt_with_template = retrieve_weblocation_template(request.prompt)
        elif detect_aad(request.prompt):
            neo4j_result = validate_structured_query(request)
            if len(neo4j_result) > 0:
                logging.info(f"Start LLM on neo4j: {neo4j_result}")
                return (
                    retrieve_neo_answer(request.prompt, neo4j_result),
                    source_document_dummy(),
                    {},
                )
            else:
                logging.info(f"Closest query: {neo4j_result}")
                prompt_with_template, reference_documents, time_stages = (
                    retrieve_answer_from_vector_store(request.prompt, chroma_filter)
                )
        else:
            neo4j_result = validate_structured_query_embedding(request)
            if len(neo4j_result) > 0:
                logging.info(f"Start LLM on neo4j: {neo4j_result}")
                return (
                    retrieve_neo_answer(request.prompt, neo4j_result),
                    source_document_dummy(),
                    {},
                )
            else:
                prompt_with_template, reference_documents, time_stages = (
                    retrieve_answer_from_vector_store(request.prompt, chroma_filter)
                )
    else:
        prompt_with_template = DEFAULT_QA_PROMPT_SIMPLE.format(question=request.prompt)
    return prompt_with_template, reference_documents, time_stages


def validate_structured_query(request: AskRequest) -> List[Dict[str, Any]]:
    """
    Query the Neo4J database using a structured (non-embedding based) approach.

    This function extracts relevant asset identifiers (AADs) from the question prompt
    using get_aad_based_on_question, and builds a Cypher query tailored to the request.
    User permissions for AADs are constructed and supplied as parameters to the query.
    If there are no relevant AADs found, an empty list is returned.
    Otherwise, the constructed Cypher query is executed on the Neo4j database.

    Args:
        request (AskRequest): The structured query request, including a prompt and permissions.

    Returns:
        list: A list of query results from the Neo4j database, or an empty list if no AADs are matched.
    """
    aads = get_aad_based_on_question(request.prompt)
    cypher_to_run = build_cypher_query(request.prompt)
    user_permissions = {
        k: list(map(str, v)) for k, v in request.permission.get("aads").items()
    }
    parameters = {"aad_ids": aads, "permissions": user_permissions}
    logging.info(f"Build cypher query: {cypher_to_run} with parameters {parameters}")
    if len(aads) == 0:
        return []
    return GRAPH.query(cypher_to_run, params=parameters)


def validate_structured_query_embedding(request: AskRequest) -> List:
    """
    Query the Neo4J database using an embedding-based approach.

    This function performs a semantic search using vector embeddings against the Chroma vector store
    for Cypher queries relevant to the user's question. The results are filtered based on associated tags
    and a similarity threshold contained in the query metadata. The top matching Cypher query is then executed
    on the Neo4J database, using user permissions (AADs and Netbeheerders) as parameters. If there are no suitable
    embedding-based matches, an empty list is returned.

    Args:
        request (AskRequest): The semantic query request, including a prompt and user permission info.

    Returns:
        list: A list containing the results of the executed Cypher query, or an empty list if no suitable queries are found.
    """
    aads = get_aad_based_on_question(request.prompt)
    nbs = check_for_nbs(request.prompt)
    results = db_cypher.similarity_search_with_relevance_scores(request.prompt, k=20)
    # NOTE: doc[0] = actual query info and doc[1] = similarity score
    tag_filtered_results = [
        doc
        for doc in results
        if match_query_by_tags(question=request.prompt, query=doc[0].metadata)
        and doc[1] > doc[0].metadata["threshold"]
    ]
    logging.info(f"Number of available querys {len(tag_filtered_results)}")
    if len(tag_filtered_results) > 0:
        top_doc, score = tag_filtered_results[0]
        cypher_to_run = top_doc.metadata["cypher"]
        logging.info(f"Closest query: {cypher_to_run} with score {score}")
        user_permissions = {
            k: list(map(str, v)) for k, v in request.permission.get("aads").items()
        }
        parameters = {
            "aad_ids": aads,
            "netbeheerders": nbs,
            "permissions": user_permissions,
        }
        logging.info(f"Parameters found: {parameters}")
        result = GRAPH.query(cypher_to_run, params=parameters)
        logging.info(f"Neo4j results: {result}")
        return result
    else:
        return []


def retrieve_neo_answer(question, neo4j_result) -> str:
    """
    Generate a formatted prompt for answering a question using Neo4j search results.

    This function processes the Neo4j search results and the original user question to produce
    a trimmed and context-aware prompt based on the Cypher query result. The generated prompt leverages
    the system's dynamic prompt templates and ensures that the context does not exceed the LLM's limits.

    Args:
        question (str): The user's original question.
        neo4j_result (Any): The result object or string returned from the Neo4j database query.

    Returns:
        str: The fully formatted prompt string suitable for LLM completion.
    """
    _, trimmed_neo4j_result = trim_context_to_fit(
        model=LLM_MANAGER.get_llm().client,
        template=DEFAULT_QA_PROMPT,
        context_text=str(neo4j_result),
        question=question,
        n_ctx=CONFIG["MAX_CTX"],
        max_tokens=CONFIG["MAX_TOKENS"],
    )
    logging.info(
        f"Trimmed neo4j result: {len(str(neo4j_result))} to {len(trimmed_neo4j_result)}"
    )
    return CYPHER_PROMPT.format(
        prompt_elementen=dynamische_prompt_elementen(question),
        result=trimmed_neo4j_result,
        question=question,
    )


def get_image_name() -> str:
    """
    Retrieve the current container image name.

    This function fetches the running container's image name from the environment
    variable 'IMAGE_NAME'. If the environment variable is not set, it returns 'Unknown'.

    Returns:
        str: The name of the Docker image or 'Unknown' if unset.
    """
    return os.getenv("IMAGE_NAME", "Unknown")


@app.post("/summarize")
async def summarize(request: FileRequest) -> Dict:
    """
    Endpoint for summarizing content provided either inline or by file path.

    This endpoint accepts a POST request with a FileRequest object, which may include direct
    text content (`content`) or a reference to a file location (`file_path`). The request is
    queued for asynchronous processing. A unique request ID and type are assigned, and status
    information including queue position and timestamps are recorded. The response includes
    a message indicating that the request is being processed, along with relevant metadata
    for tracking the request status.

    Request Body:
        FileRequest: An object that must contain either 'content' (string) for direct text
        summarization, or 'file_path' (string) to summarize the contents of a file.

    Returns:
        dict: A dictionary with a message about request processing, the request ID, request type,
        start time, and queue information.
    """
    request.id = str(uuid.uuid4())  # Genereer een uniek ID
    request.type = "summarize"
    await request_queue.put(request)  # Voeg het verzoek toe aan de wachtrij
    start_time = time.time()
    in_queue = request_queue.qsize()
    request_responses[request.id] = {
        "status": "processing",
        "start_time": start_time,
        "in_queue_start": in_queue,
        "start_time_formatted": datetime.fromtimestamp(start_time).strftime(
            "%H:%M:%S %d-%m-%Y"
        ),
    }
    return {
        "message": "Verzoek wordt verwerkt",
        "request_id": request.id,
        "type": request.type,
        "in_queue_start": in_queue,
        "start_time": datetime.fromtimestamp(start_time).strftime("%H:%M:%S %d-%m-%Y"),
    }


@app.post("/summarize/pdf")
async def summarize_pdf(file: UploadFile = File(...)) -> Dict:
    """
    Endpoint for uploading a PDF file to be summarized.

    This endpoint accepts a POST request with a single uploaded PDF file. The PDF is read in-memory,
    its text content is extracted and sanitized, and then added to the asynchronous processing queue
    for summarization, just like the /summarize endpoint, but with PDF-specific pre-processing.

    Args:
        file (UploadFile): The uploaded PDF file to be summarized.

    Returns:
        dict: A dictionary containing a message about request processing, the request ID, request type,
        original filename, queue position, and other status metadata.

    Raises:
        HTTPException: If the uploaded file is not a PDF, contains no extractable text, or cannot be read.
    """
    if file.content_type != "application/pdf":
        raise HTTPException(
            status_code=400, detail="Alleen PDF-bestanden zijn toegestaan"
        )

    # Lees bestand in geheugen
    pdf_bytes = await file.read()

    # Extraheer tekst uit PDF
    try:
        reader = PdfReader(BytesIO(pdf_bytes))
        text = ""
        for page in reader.pages:
            extracted = page.extract_text()
            if extracted:
                text += extracted + "\n"
        text = clean_for_llm(text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Fout bij lezen PDF: {e}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Geen tekst gevonden in PDF")

    # Maak FileRequest aan
    request = FileRequest(
        id=str(uuid.uuid4()),
        type="summarize",
        content=text,
        filename=file.filename,
    )

    # Queue metadata
    start_time = time.time()
    in_queue = request_queue.qsize()
    await request_queue.put(request)
    request_responses[request.id] = {
        "status": "processing",
        "start_time": start_time,
        "in_queue_start": in_queue,
        "start_time_formatted": datetime.fromtimestamp(start_time).strftime(
            "%H:%M:%S %d-%m-%Y"
        ),
    }

    return {
        "message": "PDF geüpload en toegevoegd aan samenvattingswachtrij",
        "request_id": request.id,
        "type": request.type,
        "filename": file.filename,
        "in_queue_start": in_queue,
        "start_time": datetime.fromtimestamp(start_time).strftime("%H:%M:%S %d-%m-%Y"),
    }


@app.post("/set-context")
def set_context(req: LLMRequest) -> Dict:
    """
    Endpoint to set the context window (n_ctx) for the LLM.

    This endpoint reloads the language model with a new maximum context length specified by the client.
    The context window determines how many tokens the LLM considers for each request.

    Args:
        req (LLMRequest): The request body specifying the new n_ctx value.

    Returns:
        dict: Status message and the updated context value.
    """
    LLM_MANAGER.load_llm(req.n_ctx)
    return {"status": "ok", "new_context": req.n_ctx}


@app.post("/ask")
async def ask(request: AskRequest) -> Dict:
    """
    Endpoint to handle question-answering (Q&A) requests.

    This endpoint accepts an AskRequest containing the user's question and optional metadata.
    It enqueues the request for processing and assigns a unique identifier. The function
    immediately returns queue metadata, including the generated request ID, so clients can
    poll for progress and retrieve partial or complete answers using the status endpoint.

    Args:
        request (AskRequest): The incoming question-answering request.

    Returns:
        dict: Metadata for tracking the request, including queue position and start time.
    """
    request.id = str(uuid.uuid4())  # Genereer een uniek ID
    request.type = "ask"
    await request_queue.put(request)  # Voeg het verzoek toe aan de wachtrij
    start_time = time.time()
    in_queue = request_queue.qsize()
    request_responses[request.id] = {
        "status": "processing",
        "start_time": start_time,
        "in_queue_start": in_queue,
        "start_time_formatted": datetime.fromtimestamp(start_time).strftime(
            "%H:%M:%S %d-%m-%Y"
        ),
    }
    return {
        "message": "Verzoek wordt verwerkt",
        "request_id": request.id,
        "type": request.type,
        "in_queue_start": in_queue,
        "start_time": datetime.fromtimestamp(start_time).strftime("%H:%M:%S %d-%m-%Y"),
    }


@app.get("/status/{request_id}")
async def get_status(request_id: str) -> Dict:
    """
    Endpoint to retrieve the status and result of a previously submitted request.

    This endpoint returns the current processing status, start time, position in the queue,
    and, if available, partial or complete results for an asynchronous Q&A or related request
    identified by its unique request_id.

    Args:
        request_id (str): The unique identifier of the submitted request.

    Returns:
        dict: A dictionary containing the status ("processing", "completed", or "not_found"),
            formatted start time, in-queue position, and response data if available.
    """
    if request_id in request_responses:
        response_data = request_responses[request_id]
        if response_data["status"] == "completed":
            return response_data

        if "partial_response" in response_data:
            return {
                "status": "processing",
                "start_time_formatted": response_data["start_time_formatted"],
                "in_queue_start": response_data["in_queue_start"],
                "response": {
                    "answer": response_data["partial_response"],
                    "source_documents": None,
                },
                "in_queue_current": await get_request_position_in_queue(
                    request_id=request_id
                ),
            }

        return {
            "status": "processing",
            "start_time_formatted": response_data["start_time_formatted"],
            "in_queue_start": response_data["in_queue_start"],
            "in_queue_current": await get_request_position_in_queue(
                request_id=request_id
            ),
        }
    return {"message": "Request not found", "status": "not_found"}


@app.post("/context")
def context(req: ContextRequest) -> Dict:
    """
    Endpoint to interact with the context-related functionality.

    This endpoint expects a ContextRequest object and responds with the LLM-generated answer
    to the provided prompt. Useful for submitting a prompt and receiving an immediate response.

    Args:
        req (ContextRequest): The request body containing the prompt text.

    Returns:
        dict: A dictionary with the generated answer.
    """
    return {
        "answer": LLM_MANAGER.get_llm().invoke(req.prompt),
    }


@app.get("/metadata")
def get_metadata() -> Dict:
    """
    Endpoint to fetch metadata information.

    Returns:
        dict: A dictionary containing various configuration details, such as
        the image name and other relevant metadata related to the running instance.
    """
    CONFIG["image_name"] = get_image_name()
    return CONFIG


@app.on_event("startup")
async def startup():
    """Start de worker om verzoeken sequentieel te verwerken."""
    asyncio.create_task(request_worker())
